{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transliteration task using RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SashanthMogre/colaboratory/blob/main/Transliteration_task_using_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRfy0SO7iAqX",
        "outputId": "1b7a0399-591c-49e0-cae1-38fef5673539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GqTjeV47m4B"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiates the device to be used as GPU/CPU based on availability\n",
        "device_gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "iCVLs7Z2jiM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output\n",
        "import random"
      ],
      "metadata": {
        "id": "sAAcoRVrjiYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a04ZKx7Sh-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "116016ff-f832-4ad1-8a11-3e9f4839dedc"
      },
      "source": [
        "# when we don't have same sized input characters we'll add the pad character\n",
        "eng_alphabets = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "pad_char = '-PAD-'\n",
        "\n",
        "eng_alpha2index = {pad_char: 0}\n",
        "for index, alpha in enumerate(eng_alphabets):\n",
        "    eng_alpha2index[alpha] = index+1\n",
        "\n",
        "print(eng_alpha2index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'-PAD-': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPSZsy1kXd9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b0bcb15-1472-4927-ff1e-6715fe728038"
      },
      "source": [
        "# Hindi Unicode Hex Range is 2304:2432.\n",
        "\n",
        "hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
        "hindi_alphabet_size = len(hindi_alphabets)\n",
        "\n",
        "hindi_alpha2index = {pad_char: 0}\n",
        "for index, alpha in enumerate(hindi_alphabets):\n",
        "    hindi_alpha2index[alpha] = index+1\n",
        "\n",
        "print(hindi_alpha2index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'-PAD-': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcS6ByndOxrC"
      },
      "source": [
        "# pre-processing\n",
        "import re\n",
        "non_eng_letters_regex = re.compile('[^a-zA-Z ]')\n",
        "\n",
        "# removing all english non-letters\n",
        "def cleanEnglishVocab(line):\n",
        "    line = line.replace('-', ' ').replace(',', ' ').upper()\n",
        "    line = non_eng_letters_regex.sub('', line)\n",
        "    return line.split()\n",
        "\n",
        "# removing all hindi non-letters\n",
        "def cleanHindiVocab(line):\n",
        "    line = line.replace('-', ' ').replace(',', ' ')\n",
        "    cleaned_line = ''\n",
        "    for char in line:\n",
        "        if char in hindi_alpha2index or char == ' ':\n",
        "            cleaned_line += char\n",
        "    return cleaned_line.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGSeoMGg0FTy"
      },
      "source": [
        "# for reading xml file\n",
        "from torch.utils.data import Dataset\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "class TransliterationDataLoader(Dataset):\n",
        "    def __init__(self, filename):\n",
        "        self.eng_words, self.hindi_words = self.readXmlDataset(filename, cleanHindiVocab)\n",
        "        self.shuffle_indices = list(range(len(self.eng_words)))\n",
        "        random.shuffle(self.shuffle_indices)\n",
        "        self.shuffle_start_index = 0\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.eng_words)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.eng_words[idx], self.hindi_words[idx]\n",
        "    \n",
        "    def readXmlDataset(self, filename, lang_vocab_cleaner):\n",
        "        transliterationCorpus = ET.parse(filename).getroot()\n",
        "        lang1_words = []\n",
        "        lang2_words = []\n",
        "\n",
        "        for line in transliterationCorpus:\n",
        "            wordlist1 = cleanEnglishVocab(line[0].text)\n",
        "            wordlist2 = lang_vocab_cleaner(line[1].text)\n",
        "\n",
        "            # for skipping noisy data\n",
        "            if len(wordlist1) != len(wordlist2):\n",
        "                print('Skipping: ', line[0].text, ' - ', line[1].text)\n",
        "                continue\n",
        "\n",
        "            for word in wordlist1:\n",
        "                lang1_words.append(word)\n",
        "            for word in wordlist2:\n",
        "                lang2_words.append(word)\n",
        "\n",
        "        return lang1_words, lang2_words\n",
        "    \n",
        "    def get_random_sample(self):\n",
        "        return self.__getitem__(np.random.randint(len(self.eng_words)))\n",
        "    \n",
        "    def get_batch_from_array(self, batch_size, array):\n",
        "        end = self.shuffle_start_index + batch_size\n",
        "        batch = []\n",
        "        if end >= len(self.eng_words):\n",
        "            batch = [array[i] for i in self.shuffle_indices[0:end%len(self.eng_words)]]\n",
        "            end = len(self.eng_words)\n",
        "        return batch + [array[i] for i in self.shuffle_indices[self.shuffle_start_index : end]]\n",
        "    \n",
        "    def get_batch(self, batch_size, postprocess = True):\n",
        "        eng_batch = self.get_batch_from_array(batch_size, self.eng_words)\n",
        "        hindi_batch = self.get_batch_from_array(batch_size, self.hindi_words)\n",
        "        self.shuffle_start_index += batch_size + 1\n",
        "        \n",
        "        # For reshuffling after each epoch is complete\n",
        "        if self.shuffle_start_index >= len(self.eng_words):\n",
        "            random.shuffle(self.shuffle_indices)\n",
        "            self.shuffle_start_index = 0\n",
        "            \n",
        "        return eng_batch, hindi_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FCCi-SerZS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb53749-38db-4e2e-b460-946cced5ed7d"
      },
      "source": [
        "train_data = TransliterationDataLoader('/content/drive/MyDrive/Transliteration/Dataset/NEWS2018_M-EnHi_trn.xml')\n",
        "test_data = TransliterationDataLoader('/content/drive/MyDrive/Transliteration/Dataset/NEWS2018_M-EnHi_dev.xml')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping:  australian national university  -  ऑस्ट्रेलियननेशनल यूनिवर्सिटी\n",
            "Skipping:  azamnagar road  -  आज़मनगर\n",
            "Skipping:  bal krishna  -  बालकृष्णा\n",
            "Skipping:  barharwa junction  -  बरहरवा\n",
            "Skipping:  cape town  -  केपटाउन\n",
            "Skipping:  colourplus fashions  -  कलर प्लस फ़ैशन्स\n",
            "Skipping:  dibang valley  -  दिबंगवैली\n",
            "Skipping:  envoy communications group  -  एन्वॉय कम्युनिकेशंस\n",
            "Skipping:  fakhrun nisa  -  फखरुन्निसा\n",
            "Skipping:  jahan aara  -  जहाँआरा\n",
            "Skipping:  kara-kum  -  काराकुम\n",
            "Skipping:  kelvingrove art gallery and museum  -  केल्व‍िनग्रोव आर्ट एण्ड म्युज़ियम\n",
            "Skipping:  king edward vii  -  किंग एडवर्ड\n",
            "Skipping:  londonheathrow  -  लंदन हीथ्रो\n",
            "Skipping:  mass mutual life  -  मास म्युच्युअल लाइफ़ इंश्योरेंस\n",
            "Skipping:  mauna loa  -  मौनालोआ\n",
            "Skipping:  navabharat ferro alloys  -  नव भारत फ़ैरो अलॉय\n",
            "Skipping:  netaji subhash chandra bose  -  नेताजी सुभाषचंद्र बोस\n",
            "Skipping:  new zealand  -  न्यूज़ीलैंड\n",
            "Skipping:  newfoundland  -  न्यू फाउंडलैंड\n",
            "Skipping:  omkarnath thakur  -  ओंकार नाथ ठाकुर\n",
            "Skipping:  opentv  -  ओपन टीवी\n",
            "Skipping:  order of vasa  -  ऑडर ऑफ़ द वासा\n",
            "Skipping:  paris charles de gaulle  -  पेरिस रॉसे चार्ल्स डे ग्यूले\n",
            "Skipping:  parkway apostolic  -  पार्क वे अपोस्टोलिक\n",
            "Skipping:  rama lingeshwara  -  रामालिंगेश्वर\n",
            "Skipping:  ramcoind  -  राम्को इंड\n",
            "Skipping:  rediff.com india limited  -  रेडिफ़ डॉट कॉम इंडिया लिमिटेड\n",
            "Skipping:  retalix  -  रेटालिक्स लि.\n",
            "Skipping:  rockbrook united  -  रॉकब्रुक यूनाइटेड मेथोडिस्ट\n",
            "Skipping:  sea of the hebrides  -  सी ऑफ हरब्रिड्‍स\n",
            "Skipping:  south arlington church of christ  -  साउथ अर्लिंग्टन\n",
            "Skipping:  srisailam  -  श्री शैलम\n",
            "Skipping:  state bnk tr  -  स्टेट बैंक ऑफ त्रावणकोर\n",
            "Skipping:  vaparaiso church of christ  -  व्हापरासिओ\n",
            "Skipping:  walter scott  -  वॉल्टरस्कॉट\n",
            "Skipping:  war of the holy league  -  वार ऑफ होली लीग\n",
            "Skipping:  wind river  -  विंडरिवर\n",
            "Skipping:  stats chippac  -  स्टेट्सचिपपैक\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjY06ghEx76b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1780bc79-0e1e-4a52-ffc5-84b002539b20"
      },
      "source": [
        "print(\"Train Set Size:\\t\", len(train_data))\n",
        "print(\"Test Set Size:\\t\", len(test_data))\n",
        "\n",
        "print('\\nSample data from train-set:')\n",
        "for i in range(10):\n",
        "    eng, hindi = train_data.get_random_sample()\n",
        "    print(eng + ' - ' + hindi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Set Size:\t 19115\n",
            "Test Set Size:\t 1428\n",
            "\n",
            "Sample data from train-set:\n",
            "MUSHTAQ - मुश्ताक़\n",
            "LIECHTENSTEIN - लीकटेंस्टीन\n",
            "SALVI - साल्वि\n",
            "NORMAN - नॉर्मन\n",
            "NASPERS - नैस्पर्स\n",
            "JOEL - जोएल\n",
            "JANEEVA - जनीवा\n",
            "METHUSELAH - मेथुसेलाह\n",
            "WAR - वार\n",
            "DHARAM - धरम\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE3at5C7Sy5F"
      },
      "source": [
        "# For encoding the words\n",
        "# english word\n",
        "def word_rep(word, letter2index, device = 'cpu'):\n",
        "    rep = torch.zeros(len(word)+1, 1, len(letter2index)).to(device)\n",
        "    for letter_index, letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        rep[letter_index][0][pos] = 1\n",
        "    pad_pos = letter2index[pad_char]\n",
        "    rep[letter_index+1][0][pad_pos] = 1\n",
        "    return rep\n",
        "# ground truth hindi word\n",
        "def gt_rep(word, letter2index, device = 'cpu'):\n",
        "    gt_rep = torch.zeros([len(word)+1, 1], dtype=torch.long).to(device)\n",
        "    for letter_index, letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        gt_rep[letter_index][0] = pos\n",
        "    gt_rep[letter_index+1][0] = letter2index[pad_char]\n",
        "    return gt_rep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yE3jToOrfzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc8209d-3634-44da-aec1-e74ccd12f788"
      },
      "source": [
        "eng, hindi = train_data.get_random_sample()\n",
        "eng_rep = word_rep(eng, eng_alpha2index)\n",
        "print(eng, eng_rep)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HAREE tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMcDjIberhc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f13179c-c190-434c-a636-9639c3362ce3"
      },
      "source": [
        "hindi_gt = gt_rep(hindi, hindi_alpha2index)\n",
        "print(hindi, hindi_gt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "हरी tensor([[58],\n",
            "        [49],\n",
            "        [65],\n",
            "        [ 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w8ffT3w4lkK"
      },
      "source": [
        "# Encoder - Decoder (using RNN)\n",
        "MAX_OUTPUT_CHARS = 30\n",
        "class Transliteration_EncoderDecoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, verbose=False):\n",
        "        super(Transliteration_EncoderDecoder, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.encoder_rnn_cell = nn.RNN(input_size, hidden_size)\n",
        "        self.decoder_rnn_cell = nn.RNN(output_size, hidden_size)\n",
        "        \n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        \n",
        "        self.verbose = verbose\n",
        "        \n",
        "    def forward(self, input, max_output_chars = MAX_OUTPUT_CHARS, device = 'cpu', ground_truth = None):\n",
        "        \n",
        "        # encoder\n",
        "        out, hidden = self.encoder_rnn_cell(input)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print('Encoder input', input.shape)\n",
        "            print('Encoder output', out.shape)\n",
        "            print('Encoder hidden', hidden.shape)\n",
        "        \n",
        "        # decoder\n",
        "        decoder_state = hidden\n",
        "        decoder_input = torch.zeros(1, 1, self.output_size).to(device)\n",
        "        outputs = []\n",
        "        \n",
        "        if self.verbose:\n",
        "            print('Decoder state', decoder_state.shape)\n",
        "            print('Decoder input', decoder_input.shape)\n",
        "        \n",
        "        for i in range(max_output_chars):\n",
        "            \n",
        "            out, decoder_state = self.decoder_rnn_cell(decoder_input, decoder_state)\n",
        "            \n",
        "            if self.verbose:\n",
        "                print('Decoder intermediate output', out.shape)\n",
        "            \n",
        "            out = self.h2o(decoder_state)\n",
        "            out = self.softmax(out)\n",
        "            outputs.append(out.view(1, -1))\n",
        "            \n",
        "            if self.verbose:\n",
        "                print('Decoder output', out.shape)\n",
        "                self.verbose = False\n",
        "            \n",
        "            max_idx = torch.argmax(out, 2, keepdim=True)\n",
        "            if not ground_truth is None:\n",
        "                max_idx = ground_truth[i].reshape(1, 1, 1)\n",
        "            one_hot = torch.FloatTensor(out.shape).to(device)\n",
        "            one_hot.zero_()\n",
        "            one_hot.scatter_(2, max_idx, 1)\n",
        "            \n",
        "            decoder_input = one_hot.detach()\n",
        "            \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cra9toTiOoPm"
      },
      "source": [
        "net = Transliteration_EncoderDecoder(len(eng_alpha2index), 256, len(hindi_alpha2index), verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_pdzBmQOsjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e575ab-edf0-4b27-ea58-23e939e0baf6"
      },
      "source": [
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transliteration_EncoderDecoder(\n",
            "  (encoder_rnn_cell): RNN(27, 256)\n",
            "  (decoder_rnn_cell): RNN(129, 256)\n",
            "  (h2o): Linear(in_features=256, out_features=129, bias=True)\n",
            "  (softmax): LogSoftmax(dim=2)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z-1QDAz8F_d"
      },
      "source": [
        "# Encoder-Decoder with attention\n",
        "MAX_OUTPUT_CHARS = 30\n",
        "class Transliteration_EncoderDecoder_Attention(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, verbose=False):\n",
        "        super(Transliteration_EncoderDecoder_Attention, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.encoder_rnn_cell = nn.RNN(input_size, hidden_size)\n",
        "        self.decoder_rnn_cell = nn.RNN(hidden_size*2, hidden_size)\n",
        "        \n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        \n",
        "        self.U = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.W = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size, 1)\n",
        "        self.out2hidden = nn.Linear(self.output_size, self.hidden_size)   \n",
        "        \n",
        "        self.verbose = verbose\n",
        "        \n",
        "    def forward(self, input, max_output_chars = MAX_OUTPUT_CHARS, device = 'cpu', ground_truth = None):\n",
        "        \n",
        "        # encoder\n",
        "        encoder_outputs, hidden = self.encoder_rnn_cell(input)\n",
        "        encoder_outputs = encoder_outputs.view(-1, self.hidden_size)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print('Encoder output', encoder_outputs.shape)\n",
        "        \n",
        "        # decoder\n",
        "        decoder_state = hidden\n",
        "        decoder_input = torch.zeros(1, 1, self.output_size).to(device)\n",
        "        \n",
        "        outputs = []\n",
        "        U = self.U(encoder_outputs)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print('Decoder state', decoder_state.shape)\n",
        "            print('Decoder intermediate input', decoder_input.shape)\n",
        "            print('U * Encoder output', U.shape)\n",
        "        \n",
        "        for i in range(max_output_chars):\n",
        "            \n",
        "            W = self.W(decoder_state.view(1, -1).repeat(encoder_outputs.shape[0], 1))\n",
        "            V = self.attn(torch.tanh(U + W))\n",
        "            attn_weights = F.softmax(V.view(1, -1), dim = 1) \n",
        "            \n",
        "            if self.verbose:\n",
        "                print('W * Decoder state', W.shape)\n",
        "                print('V', V.shape)\n",
        "                print('Attn', attn_weights.shape)\n",
        "            \n",
        "            attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "            \n",
        "            embedding = self.out2hidden(decoder_input)\n",
        "            decoder_input = torch.cat((embedding[0], attn_applied[0]), 1).unsqueeze(0)\n",
        "            \n",
        "            if self.verbose:\n",
        "                print('Attn LC', attn_applied.shape)\n",
        "                print('Decoder input', decoder_input.shape)\n",
        "                \n",
        "            out, decoder_state = self.decoder_rnn_cell(decoder_input, decoder_state)\n",
        "            \n",
        "            if self.verbose:\n",
        "                print('Decoder intermediate output', out.shape)\n",
        "                \n",
        "            out = self.h2o(decoder_state)\n",
        "            out = self.softmax(out)\n",
        "            outputs.append(out.view(1, -1))\n",
        "            \n",
        "            if self.verbose:\n",
        "                print('Decoder output', out.shape)\n",
        "                self.verbose = False\n",
        "            \n",
        "            max_idx = torch.argmax(out, 2, keepdim=True)\n",
        "            if not ground_truth is None:\n",
        "                max_idx = ground_truth[i].reshape(1, 1, 1)\n",
        "            one_hot = torch.zeros(out.shape, device=device)\n",
        "            one_hot.scatter_(2, max_idx, 1) \n",
        "            \n",
        "            decoder_input = one_hot.detach()\n",
        "            \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMD3zjdJO0Oj"
      },
      "source": [
        "net_attn = Transliteration_EncoderDecoder_Attention(len(eng_alpha2index), 256, len(hindi_alpha2index), verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUyGckhZp4Wm"
      },
      "source": [
        "def infer(net, word, max_chars):\n",
        "  engrep = word_rep(word, eng_alpha2index)\n",
        "  return net(engrep,max_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoiQwbntO5UH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6b0aa0-f7de-4d4f-95ce-53506f924cd4"
      },
      "source": [
        "out = infer(net_attn, 'INDIA', 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output torch.Size([6, 256])\n",
            "Decoder state torch.Size([1, 1, 256])\n",
            "Decoder intermediate input torch.Size([1, 1, 129])\n",
            "U * Encoder output torch.Size([6, 256])\n",
            "W * Decoder state torch.Size([6, 256])\n",
            "V torch.Size([6, 1])\n",
            "Attn torch.Size([1, 6])\n",
            "Attn LC torch.Size([1, 1, 256])\n",
            "Decoder input torch.Size([1, 1, 512])\n",
            "Decoder intermediate output torch.Size([1, 1, 256])\n",
            "Decoder output torch.Size([1, 1, 129])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9WSPgzlO6k8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a0781d-c3ef-4b49-bfdc-f803436f4881"
      },
      "source": [
        "print(len(out))\n",
        "for i in range(len(out)):\n",
        "    print(out[i].shape, list(hindi_alpha2index.keys())[list(hindi_alpha2index.values()).index(torch.argmax(out[i]))])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "torch.Size([1, 129]) द\n",
            "torch.Size([1, 129]) ऎ\n",
            "torch.Size([1, 129]) ऎ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n",
            "torch.Size([1, 129]) ॆ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m804jsH7AXSV"
      },
      "source": [
        "# Training\n",
        "def train_batch(net, opt, criterion, batch_size, device = 'cpu', teacher_force = False):\n",
        "    \n",
        "    net.train().to(device)\n",
        "    opt.zero_grad()\n",
        "    eng_batch, hindi_batch = train_data.get_batch(batch_size)\n",
        "    \n",
        "    total_loss = 0\n",
        "    for i in range(batch_size):\n",
        "        \n",
        "        input = word_rep(eng_batch[i], eng_alpha2index, device)\n",
        "        gt = gt_rep(hindi_batch[i], hindi_alpha2index, device)\n",
        "        outputs = net(input, gt.shape[0], device, ground_truth = gt if teacher_force else None)\n",
        "        \n",
        "        for index, output in enumerate(outputs):\n",
        "            loss = criterion(output, gt[index]) / batch_size\n",
        "            loss.backward(retain_graph = True)\n",
        "            total_loss += loss\n",
        "        \n",
        "    opt.step()\n",
        "    return total_loss/batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rjto129ssrpr"
      },
      "source": [
        "def train_setup(net, lr = 0.01, n_batches = 100, batch_size = 10, momentum = 0.9, display_freq=5, device = 'cpu'):\n",
        "    \n",
        "    net = net.to(device)\n",
        "    criterion = nn.NLLLoss(ignore_index = -1)\n",
        "    opt = optim.Adam(net.parameters(), lr=lr)\n",
        "    teacher_force_upto = n_batches//3\n",
        "    \n",
        "    loss_arr = np.zeros(n_batches + 1)\n",
        "    \n",
        "    for i in range(n_batches):\n",
        "        loss_arr[i+1] = (loss_arr[i]*i + train_batch(net, opt, criterion, batch_size, device = device, teacher_force = i<teacher_force_upto ))/(i + 1)\n",
        "        \n",
        "        if i%display_freq == display_freq-1:\n",
        "            clear_output(wait=True)\n",
        "            \n",
        "            print('Iteration', i, 'Loss', loss_arr[i])\n",
        "            plt.figure()\n",
        "            plt.plot(loss_arr[1:i], '-*')\n",
        "            plt.xlabel('Iteration')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.show()\n",
        "            print('\\n\\n')\n",
        "            \n",
        "    torch.save(net, 'model.pt')\n",
        "    return loss_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oQ3ZIWvtjfN"
      },
      "source": [
        "# Training without attention\n",
        "net = Transliteration_EncoderDecoder(len(eng_alpha2index), 256, len(hindi_alpha2index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6LjVKQfoVMU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "c49e816d-ef2a-4270-8f3c-ddbc3ba9e73a"
      },
      "source": [
        "train_setup(net, lr=0.001, n_batches=2000, batch_size = 64, display_freq=10, device = device_gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4017ef4cceaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-a6aff6a6c25d>\u001b[0m in \u001b[0;36mtrain_setup\u001b[0;34m(net, lr, n_batches, batch_size, momentum, display_freq, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Resets _flat_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net.state_dict(), 'checkpoint3.pth')\n",
        "\n",
        "# download checkpoint file\n",
        "#files.download('checkpoint.pth')"
      ],
      "metadata": {
        "id": "RpnTlyhtJque"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import helper\n",
        "\n"
      ],
      "metadata": {
        "id": "s79fxuF1WiIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('checkpoint3.pth')"
      ],
      "metadata": {
        "id": "VwwKZuoWXcQY",
        "outputId": "8af6e7f0-7e74-4b16-9aa0-85b9ace990c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-1829eb9b7a3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint3.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: checkpoint3.pth"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QcCqQGTsXbwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.load_state_dict(torch.load('checkpoint3.pth'))\n",
        "net.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "QoqEl5W7VxsP",
        "outputId": "944f5fca-d949-45b4-8e75-0f68cc8df1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e1a3e6401df3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint3.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoint3.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxFLBqW1Ip4v"
      },
      "source": [
        "# Training with attention\n",
        "net_att = Transliteration_EncoderDecoder_Attention(len(eng_alpha2index), 256, len(hindi_alpha2index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdRpJUXNIwuv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "8621c88f-e594-4cd2-ef3f-02a82cf2cd80"
      },
      "source": [
        "loss_history = train_setup(net_att, lr=0.001, n_batches=2000, batch_size = 64, display_freq=10, device = device_gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1999 Loss 0.14892005920410156\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5RcZZnv8e8vHZIgJAgmQQiBDkyCBkTANoiAgtwCcRLUdTSAGTzqAkbjfQ4E4cw4KEPApUtE5pgcD6Mjl4hwYHJOwCg3Bc+QpIPhkjAhIURIDNBc5DKE3HjOH3tXs9NWdXd1166q7v37rFWra79776qnd3fX0+9lv68iAjMzs66GNDoAMzNrTk4QZmZWlhOEmZmV5QRhZmZlOUGYmVlZQxsdQK2MHj06WltbGx2GmdmAsnz58ucjYky5fYMmQbS2ttLe3t7oMMzMBhRJf6y0z01MZmZWlhOEmZmV5QRhZmZlOUGYmVlZThBmZlaWEwTw3Ctv8Ml5/85zr77R6FDMzJqGEwTww7vWsGz9i/zwzjWNDsXMrGkMmvsg+uLgS+5gy/Y3O7evW/IU1y15iuFDh7D6O6c1MDIzs8YrdA3ivgtOYOqh7+zcHrHLEGYcvi/3XXhCA6MyM2sOhU4QY0eNYOTwpBI1dIjYsv1NRg4fytiRIxocmZlZ4xW6iQngz5u3AnD2UfuzI6DDHdVmZoATBPNntTHhott5+9uG8bWTJzU6HDOzplHoJiYASQwdAje1P+1hrmZmGYVPECWbXn7Dw1zNzDIK3cTkYa5mZpUVugZx3wUnMP3wfTu3PczVzOwthU4Q2WGuQ4SHuZqZZeSaICRNlbRa0lpJc8rs/4ykDkkr0sfnM/vOkbQmfZyTV4zPv7aFkcOHcuxfjebsow6g47Uteb2VmdmAklsfhKQW4BrgZGADsEzSwohY1eXQX0TE7C7n7gX8A9AGBLA8PfelWsc5b1YbU3/wO0bs0sJ3zji01i9vZjZg5VmDmAKsjYh1EbEVWADM6OW5pwK/iYgX06TwG2BqTnECsOTJFz3M1cwsI88EMQ54OrO9IS3r6hOSHpZ0s6Tx1Zwr6VxJ7ZLaOzo6+hzo869t4eXN2zzM1cwso9HDXP8PcGNEbJF0HvAz4CO9PTki5gPzAdra2qLaN/cwVzOzyvKsQWwExme290vLOkXECxFR6hX+CfC+3p5bC6VhrkOUbHuYq5nZW/JMEMuAiZImSBoGzAQWZg+QtE9mczrwWPp8MXCKpD0l7QmckpbVVGmY65sBwsNczcyycmtiiojtkmaTfLC3ANdGxEpJlwLtEbEQ+LKk6cB24EXgM+m5L0r6NkmSAbg0Il7MI87nX9vC/nvtSgR8+OCxns3VzCyliKqb7ptSW1tbtLe39+nc83/ezj2rO7jvwhNcezCzQpG0PCLayu0r9J3UJWuee40t29/0KCYzs4xGj2JqKI9iMjOrrNA1iNIoppZ0GJNHMZmZvaXQCaI0imnHm0k/jEcxmZm9pdAJApJRTO8ZNwqAs6fs78n6zMxShe6DgGSyvqvvWsMjG1/hW9MPYWhL4XOmmRngGgRAZ0f1xj9vbnAkZmbNwwkC+Pd1LwDwo7vXNjgSM7PmUegmpq7DXH+5fAO/XL7Bw1zNzCh4DaI0zHWXlmSY6/ChHuZqZlZS6ARRGua6fUcyzHWrh7mamXUqdIKAZJjrsRNHAzDtsH08zNXMLFX4BDFvVhvT37svAJ9833jmzSo7Z5WZWeEUPkEA3P7IJgCuX/rHBkdiZtY8PIopM4pp8cpnaZ2zyKOYzMwoeA2iNIpp2NDkMgxr8SgmM7OSQieI0iimbWktYuuONxkqeRSTmRk5JwhJUyWtlrRW0pxujvuEpJDUlm63StosaUX6+HFeMT7/2hamHfbW0thL1+eysqmZ2YCTWx+EpBbgGuBkYAOwTNLCiFjV5biRwFeAJV1e4omIODyv+EruXd2xUz/E0y9tdj+EmRn51iCmAGsjYl1EbAUWADPKHPdt4ArgjRxjqei+C07gpHeP7dz2okFmZok8E8Q44OnM9oa0rJOkI4HxEbGozPkTJP1B0m8lHVfuDSSdK6ldUntHR0efghw7agSjRuwCwNAh8qJBZmaphg1zlTQE+D7wmTK7NwH7R8QLkt4H3CbpkIh4JXtQRMwH5gO0tbVFX2P58+tbARgzcjgfPGi076Y2MyPfGsRGYHxme7+0rGQkcChwr6T1wAeAhZLaImJLRLwAEBHLgSeASXkFevVZRwLwzMtvsOsuQ3w3tZkZ+dYglgETJU0gSQwzgbNKOyPiZWB0aVvSvcDfRUS7pDHAixGxQ9KBwERgXR5BZm+WC+C6JU9x3ZKn3EltZoWXWw0iIrYDs4HFwGPATRGxUtKlkqb3cPqHgIclrQBuBs6PiFzGn953wQmdczGBO6nNzEpy7YOIiNuB27uU/X2FY4/PPL8FuCXP2ErGjhrByBHJZWiRO6nNzEoKPRdTyfOvbWFYi9ht+FA+8q693UltZkbBp9oomTerjV1ahvDS69vcSW1mlip8DaLrjK7upDYzSxS+BlGa0VXptjupzcwShU8QpRldA5BwJ7WZWarwTUyQdFKP2X0Yr23Zwenv8brUZmbgGgSQdFIPGzqEzdt2uJPazCxV+BqEO6nNzMorfA2i1EndkvZSu5PazCxR+ARR6qTekc4F+8Y2d1KbmYETBJB0Ur9912RNiIljd3cntZkZThAcfMkdLF75LH/evA2ANc+9xuKVz3LwJXc0ODIzs8YqfILoeqNci3AfhJkZHsXEcVfes9Moph0B/7biT/zq0Wc8isnMCs01iAtO4J17DO+sQQwB9tljhGsQZlZ4hU8QY0eN4MR37U1pQes3gRPfNdajmMys8AqfIA6+5A6uX/LUTmXXLXnKndRmVniFTxClTuphLcmlEHDqIXu7icnMCi/XBCFpqqTVktZKmtPNcZ+QFJLaMmUXpeetlnRqXjGWbpTbtiPpqA5gXcd/uonJzAovt1FMklqAa4CTgQ3AMkkLI2JVl+NGAl8BlmTKJgMzgUOAfYE7JU2KiB15xHrj0qc6+yAguReidc4iz8dkZoWWZw1iCrA2ItZFxFZgATCjzHHfBq4A3siUzQAWRMSWiHgSWJu+Xi4euOhETjv0nZ3bw4fK90KYWeHlmSDGAU9ntjekZZ0kHQmMj4hF1Z6bnn+upHZJ7R0dHX0OdOyoEez5tmGd21u2h+djMrPCa1gntaQhwPeBb/T1NSJifkS0RUTbmDFj+hzLwZfcwQ1LPZLJzCwrzwSxERif2d4vLSsZCRwK3CtpPfABYGHaUd3TuTVVGslU4uk2zMzyTRDLgImSJkgaRtLpvLC0MyJejojREdEaEa3AA8D0iGhPj5spabikCcBEYGlegR535T0sXPGnzu3SdBvHXXFPXm9pZtb0cksQEbEdmA0sBh4DboqIlZIulTS9h3NXAjcBq4BfAV/MawQTvDXdRpan2zCzolNE9HzUANDW1hbt7e19OrfrsqMlHuZqZoOdpOUR0VZuX+HvpAbXIMzMynGCIOmDeOblnVeR2/TyG+6DMLNCc4IAKrWyDY7GNzOzvnGCAO6/8ITO9SCytm5/0/dCmFlhOUGQ3EldqbbgWoSZFZUTROr4SaPZffjOcxe2vuNt3O+OajMrKCeI1L2PP89rW7bvVLb+hdeZctldDYrIzKyxnCBSu7SU64WoXG5mNtg5QaS27Sjf21Cp3MxssHOCSN3+5WMr7mud03U2cjOzwc8JIjV53z0aHYKZWVNxgugl3w9hZkXjBJGx9JsnVtxXbjI/M7PBzAkiY+yoykuMejSTmRWNE0QveTSTmRWNE0QXvh/CzCzhBNHF7y/8SNnybTvCw13NrFByTRCSpkpaLWmtpDll9p8v6RFJKyTdL2lyWt4qaXNavkLSj/OMM6u7fggzsyLJLUFIagGuAU4DJgNnlhJAxg0R8Z6IOBy4Evh+Zt8TEXF4+jg/rzjLUTetSa5FmFlR5FmDmAKsjYh1EbEVWADMyB4QEa9kNnejSWbXXnJR5eGuZmZF0asEIWk3SUPS55MkTZe0Sw+njQOezmxvSMu6vvYXJT1BUoP4cmbXBEl/kPRbScdViOtcSe2S2js6OnrzrfRKT81MvmnOzIqgtzWI3wEjJI0Dfg3MAn5aiwAi4pqIOAi4ELgkLd4E7B8RRwBfB26QNKrMufMjoi0i2saMGVOLcDodfeBeFff5pjkzK4LeJghFxOvAx4F/joj/AhzSwzkbgfGZ7f3SskoWAGcARMSWiHghfb4ceAKY1MtYa+LGc4/udr/7IsxssOt1gpB0NHA2UPpkbOnhnGXAREkTJA0DZgILu7zoxMzmNGBNWj4m7eRG0oHARGBdL2OtmXfsNqzeb2lm1jR6myC+ClwE3BoRK9MP7Xu6OyEitgOzgcXAY8BN6bmXSpqeHjZb0kpJK0iaks5Jyz8EPJyW3wycHxEvVvWd1cDy/35yt/tdizCzwUwR1Q0cSjurd+8yAqnh2traor29veav+5lrl3Dv489X3L9+7rSav6eZWb1IWh4RbeX29XYU0w2SRknaDXgUWCXpv9UyyGb1088e1e1+1yLMbLDqbRPT5LTGcAZwBzCBZCRTIXQ3ogk87NXMBqfeJohd0vsezgAWRsQ2muSmtnroaUSTh72a2WDU2wQxD1hPcrfz7yQdADRVH0Texo4c3u1+NzWZ2WDTqwQRET+MiHERcXok/gickHNsTWXpxSfhGb/NrEh620m9h6Tvl6a1kPQ9ktpEoTxxefcjllyLMLPBpLdNTNcCrwKfTB+vAP+SV1DNrKeFg5wkzGyw6G2COCgi/iGdmXVdRPwjcGCegTWrNZed3uMxHtVkZoNBbxPEZknHljYkHQNsziek5nfqIXt3u9+jmsxsMOhtgjgfuEbSeknrgR8B5+UWVZObN6ut20WFwE1NZjbw9XYU00MR8V7gMOCwdBru8os3F8STPXRYg5OEmQ1sVa0oFxGvZOZg+noO8QwovZmHyUnCzAaq/iw56rsC6Lk/ApwkzGxg6k+CKMxUG92ZN6vNScLMBqVuE4SkVyW9UubxKrBvnWJsevNmtfU4FQckSWLVppfrEJGZWf91myAiYmREjCrzGBkRQ+sV5ECw9OKTenXc6Vfdz7zfrsk5GjOz/utPE5N1sX7utF41N11+x+O0zlnE/Ws76hCVmVnfOEHUWG+bmwA+/ZOlTHCiMLMmlWuCkDRV0mpJayXNKbP/fEmPSFoh6X5JkzP7LkrPWy3p1DzjrLWlF5/Uq5oEJD39ThRm1oyqXpO61y8stQCPAycDG4BlwJkRsSpzzKjSfRWSpgNfiIipaaK4EZhC0hl+JzApInZUer+81qTur76MXvqnjx3CWUe11j4YM7Mu+r0mdR9NAdamk/ttBRYAM7IHZG66g2T68FK2mgEsiIgtEfEksDZ9vQGnNzfTdfXNW1fSOmeRO7PNrKHyTBDjgKcz2xvSsp1I+qKkJ4ArgS9Xee65pTUqOjqat3mmt53XXZU6s1vnLOKGJetrH5iZWTca3kkdEddExEHAhcAlVZ47PyLaIqJtzJgx+QRYI/NmtbF+7rQeJ/mrpFSr+ODld/Hcq2/UNjgzszLyTBAbgfGZ7f3SskoWAGf08dwB48nLp/UrUfzp5TeYctld7tQ2s9zlmSCWARMlTZA0DJgJLMweIGliZnMaUGp0XwjMlDRc0gRgIrA0x1jrrr+JojT6qXXOIr7368dqGpuZGUBud0NHxHZJs4HFQAtwbUSslHQp0B4RC4HZkk4CtgEvAeek566UdBOwCtgOfLG7EUwDWWna8CmX3clzr27p02tcffc6rr57HcNaxG2zj2HyPnvUMkQzK6jchrnWW7MOc+2L/k7sJ+Dnn5/CsX/V3P0yZtZ43Q1zdYJoYv2pVZT4ngoz644TxCDQ31rFRadN4rwPT+z5QDMrFCeIQWTCRYvoz4/sSx85kG+c8u7aBWRmA5oTxCDVn1rF8BZxqzu0zQrPCWKQ60+twonCrNicIAqiv81P7tA2Kx4niILp7+gnd2ibFYcTREGd9/N2Fq98ts/nO1GYDX5OENavDu3xe+7KLV/4IGNHjqhhRGbWDJwgrJMThZllOUHYX3CiMDNo3Ipy1sTWz01mkx07cnjV5z790mamXHaXV7wzG+RcgzCgfx3aPzrrcD562F8s+GdmA4CbmKwqfWl+8gyyZgOTm5isKqXmp2qUFjBys5PZ4OEahPWoLzWK61ybMBsQXIOwfulLjeLTP1nKDUvW5xOQmdVFrglC0lRJqyWtlTSnzP6vS1ol6WFJd0k6ILNvh6QV6WNh13Ot/tbPncaph+zd6+O/eetKWucs4rlX38gxKjPLS24JQlILcA1wGjAZOFPS5C6H/QFoi4jDgJuBKzP7NkfE4eljel5xWnXmzWpj/dxpDBva+1+dKZfd5dqE2QCUZw1iCrA2ItZFxFZgATAje0BE3BMRr6ebDwD75RiP1dDj3zmN9XOnIfXueNcmzAaePBPEOODpzPaGtKySzwF3ZLZHSGqX9ICkM8qdIOnc9Jj2jo6O/kdsVXvy8ur6J1ybMBs4mqKTWtKngTbgu5niA9Ke9bOAH0g6qOt5ETE/Itoiom3MGI+YaaS+1CZWbXo536DMrF/yTBAbgfGZ7f3Ssp1IOgm4GJgeEZ2LGETExvTrOuBe4IgcY7UaePLy6jqxT7/qftcmzJpYngliGTBR0gRJw4CZwE6jkSQdAcwjSQ7PZcr3lDQ8fT4aOAZYlWOsViOlTuzecm3CrHnlliAiYjswG1gMPAbcFBErJV0qqTQq6bvA7sAvuwxnfTfQLukh4B5gbkQ4QQwg1U4E6NqEWfPxndSWu2rvxL79K8cyeZ89corGzLJ8J7U1VLU32J1+1f3834f/orvKzOrMNQirq2prE0svPtELE5nlyNN9W1Pp69oTXnfCrPacIKwpTbhoEX359fPaE2a14z4Ia0pPXt63JU9La0+0zlnEcVfc7ek7zHLiGoQ1hb6sOdHVlz5yIN845d01iMasOFyDsKZX7Qyx5Vx99zpa5yyidc4ij4IyqwHXIKzp1KI2UbLvHiO4bfYxHgllVoE7qW3AmnLZnTz36paeD+yFi06bxHkfnliT1zIbLJwgbFDo66incjxk1izhBGGDTq2aoTxk1orOCcIGNScLs75zgrBCqGV/BbgZyorBCcIKp5YjoQD+6WOHcNZRrTV9TbNm4ARhheZkYVaZE4QZMOmSO9i6/c2avub4PXflli980PdZ2IDlBGHWRS2HzJYMaxG3zT7Gix3ZgOIEYdaNPGoW4E5uGxgaliAkTQWuAlqAn0TE3C77vw58HtgOdACfjYg/pvvOAS5JD/1ORPysu/dygrBayCtZeCJBa1YNSRCSWoDHgZOBDcAy4MyIWJU55gRgSUS8LulvgeMj4lOS9gLagTaS2Z2XA++LiJcqvZ8ThNVarYfNZrmj25pFoxLE0cC3IuLUdPsigIi4vMLxRwA/iohjJJ1JkizOS/fNA+6NiBsrvZ8ThOWpr6vg9YZv0LNG6i5BDM3xfccBT2e2NwBHdXP854A7ujn3LxpzJZ0LnAuw//779ydWs27Nm7Xz308th86WFkAqccKwZpFngug1SZ8maU76cDXnRcR8YD4kNYgcQjMra/3caZ3Paz0iygnDmkWeCWIjMD6zvV9athNJJwEXAx+OiC2Zc4/vcu69uURp1k9PXv5Wssijk9sJwxolzz6IoSSd1CeSfOAvA86KiJWZY44AbgamRsSaTPleJB3TR6ZFD5J0Ur9Y6f3cB2HNqNZ3cVfitS6srxo5zPV04Ackw1yvjYjLJF0KtEfEQkl3Au8BNqWnPBUR09NzPwt8My2/LCL+pbv3coKwZpfnqKiufIe39ZZvlDNrQnndc1GJb9yzcpwgzAaAeicM8P0Y5gRhNiDlMV9UT4a3iFs9n1ShOEGYDQKNSBjgDvDBzgnCbJCq1yiprjxz7eDhBGFWEI1KGCXuCB94nCDMCqqeQ2sr8Y19zc0Jwsw6NUPSAHeINwsnCDPrVqObprK8dkZ9OUGYWdWaKWmAm6ry4gRhZjXRqKG23dl3jxHcNvsYTyvSR04QZpabZunTKMejqnrmBGFmddeMtY0S38fxFicIM2sazda3UU6Rah5OEGbW9AZC4igZTJMcOkGY2YDVzE1VlQykGogThJkNOgOpxtFVMyUQJwgzK4xmHlXVW/WcQbeRS45OBa4iWXL0JxExt8v+D5EsSXoYMDMibs7s2wE8km52LkVaiROEmfVkIDZXVVKrfpCGJAhJLcDjwMnABmAZcGZErMoc0wqMAv4OWNglQbwWEbv39v2cIMysPwZ6zaOvzVbdJYih/Y6qsinA2ohYlwaxAJgBdCaIiFif7qvvOotmZl0svfikbvc3ewL52i8eqnm/Rp4JYhzwdGZ7A3BUFeePkNQObAfmRsRtXQ+QdC5wLsD+++/fj1DNzLrX7Alk247o7LhfP3daTV4zzwTRXwdExEZJBwJ3S3okIp7IHhAR84H5kDQxNSJIMzPoOYFMuuQOtm7Pt7FEwNVnHV6z18szQWwExme290vLeiUiNqZf10m6FzgCeKLbk8zMmtTj3zmtx2P6O3R3aItq2syUZ4JYBkyUNIEkMcwEzurNiZL2BF6PiC2SRgPHAFfmFqmZWRPoqWnovJ+3s3jlsxX3b3+ztg0puSWIiNguaTawmGSY67URsVLSpUB7RCyU9H7gVmBP4K8l/WNEHAK8G5iXdl4PIemDWFXhrczMCmHerLKDjXLjG+XMzAqsu2GuQ+odjJmZDQxOEGZmVpYThJmZleUEYWZmZTlBmJlZWYNmFJOkDuCP/XiJ0cDzNQqnlhxXdRxXdRxXdQZjXAdExJhyOwZNgugvSe2Vhno1kuOqjuOqjuOqTtHichOTmZmV5QRhZmZlOUG8ZX6jA6jAcVXHcVXHcVWnUHG5D8LMzMpyDcLMzMpygjAzs7IKnyAkTZW0WtJaSXPq/N7jJd0jaZWklZK+kpZ/S9JGSSvSx+mZcy5KY10t6dQcY1sv6ZH0/dvTsr0k/UbSmvTrnmm5JP0wjethSUfmFNPBmWuyQtIrkr7aqOsl6VpJz0l6NFNW9TWSdE56/BpJ5+QU13cl/Uf63rdKenta3ippc+ba/ThzzvvS34G1aezKIa6qf3a1/putENcvMjGtl7QiLa/n9ar0+VC/37GIKOyDZJ2KJ4ADgWHAQ8DkOr7/PsCR6fORwOPAZOBbwN+VOX5yGuNwYEIae0tOsa0HRncpuxKYkz6fA1yRPj8duINkxcMPAEvq9LN7BjigUdcL+BBwJPBoX68RsBewLv26Z/p8zxziOgUYmj6/IhNXa/a4Lq+zNI1Vaeyn5RBXVT+7PP5my8XVZf/3gL9vwPWq9PlQt9+xotcgpgBrI2JdRGwFFgAz6vXmEbEpIh5Mn78KPAZ0t17gDGBBRGyJiCeBtSTfQ73MAH6WPv8ZcEam/F8j8QDwdkn75BzLicATEdHd3fO5Xq+I+B3wYpn3rOYanQr8JiJejIiXgN8AU2sdV0T8OiK2p5sPkCwBXFEa26iIeCCST5l/zXwvNYurG5V+djX/m+0urrQW8Engxu5eI6frVenzoW6/Y0VPEOOApzPbG+j+Azo3klpJ1t1ekhbNTquJ15aqkNQ33gB+LWm5pHPTsr0jYlP6/Blg7wbEVTKTnf9oG329Sqq9Ro2I8bMk/2mWTJD0B0m/lXRcWjYujaUecVXzs6v39ToOeDYi1mTK6n69unw+1O13rOgJoilI2h24BfhqRLwC/A/gIOBwYBNJFbfejo2II4HTgC9K+lB2Z/pfUkPGSEsaBkwHfpkWNcP1+guNvEaVSLoY2A5cnxZtAvaPiCOArwM3SBpVx5Ca8meXcSY7/yNS9+tV5vOhU96/Y0VPEBuB8Znt/dKyupG0C8kP//qI+N8AEfFsROyIiDeB/8lbzSJ1izciNqZfnyNZN3wK8Gyp6Sj9+ly940qdBjwYEc+mMTb8emVUe43qFqOkzwAfBc5OP1hIm3BeSJ8vJ2nfn5TGkG2GyiWuPvzs6nm9hgIfB36Ribeu16vc5wN1/B0reoJYBkyUNCH9r3QmsLBeb562b/4v4LGI+H6mPNt+/zGgNLpiITBT0nBJE4CJJB1jtY5rN0kjS89JOjgfTd+/NALiHODfMnH9TTqK4gPAy5kqcB52+q+u0deri2qv0WLgFEl7ps0rp6RlNSVpKnABMD0iXs+Uj5HUkj4/kOQarUtje0XSB9Lf07/JfC+1jKvan109/2ZPAv4jIjqbjup5vSp9PlDP37H+9LIPhgdJz//jJP8JXFzn9z6WpHr4MLAifZwO/Bx4JC1fCOyTOefiNNbV9HOURDdxHUgyOuQhYGXpugDvAO4C1gB3Anul5QKuSeN6BGjL8ZrtBrwA7JEpa8j1IklSm4BtJO26n+vLNSLpE1ibPv5rTnGtJWmHLv2e/Tg99hPpz3gF8CDw15nXaSP5wH4C+BHpzAs1jqvqn12t/2bLxZWW/xQ4v8ux9bxelT4f6vY75qk2zMysrKI3MZmZWQVOEGZmVpYThJmZleUEYWZmZTlBmJlZWU4QZmVIei392irprBq/9je7bP+/Wr6+Wa04QZh1rxWoKkGkd+B2Z6cEEREfrDIms7pwgjDr3lzgOCVz/39NUouStRWWpRPMnQcg6XhJ90laCKxKy25LJztcWZrwUNJcYNf09a5Py0q1FaWv/aiSdQU+lXnteyXdrGRNh+vTu2zNctXTfzpmRTeHZL2CjwKkH/QvR8T7JQ0Hfi/p1+mxRwKHRjI9NcBnI+JFSbsCyyTdEhFzJM2OiMPLvNfHSSatey8wOj3nd+m+I4BDgD8BvweOAe6v/bdr9hbXIMyqcwrJfDcrSKZefgfJfDwASzPJAeDLkh4iWX9hfOa4So4Fboxk8rpngd8C78+89oZIJrVbQdL0ZZYr1yDMqiPgSxGx02Rnko4H/rPL9knA0RHxuqR7gRH9eN8tmec78N+u1YFrEGbde5VkuceSxcDfptMwI2lSOuNtV3sAL6XJ4V0kS43e0igAAACNSURBVECWbCud38V9wKfSfo4xJEth5j37rFlF/i/ErHsPAzvSpqKfAleRNO88mHYUd1B+aclfAedLeoxkNtIHMvvmAw9LejAizs6U3wocTTKLbgAXRMQzaYIxqzvP5mpmZmW5icnMzMpygjAzs7KcIMzMrCwnCDMzK8sJwszMynKCMDOzspwgzMysrP8PQXr64CZeZ+QAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net_att.state_dict(),'model3 .pth')"
      ],
      "metadata": {
        "id": "HbRDjlmbGEh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AcpQXjjhGrUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNpECrnatPJw"
      },
      "source": [
        "#loss_history = train_setup(net_att, lr=0.001, n_batches=3000, batch_size = 64, display_freq=10, device = device_gpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3TWC7zhAn3z"
      },
      "source": [
        "def test(net, word, device = 'cpu'):\n",
        "    net = net.eval().to(device)\n",
        "    outputs = infer(net, word, 30)\n",
        "    hindi_output = ''\n",
        "    for out in outputs:\n",
        "        val, indices = out.topk(1)\n",
        "        index = indices.tolist()[0][0]\n",
        "        if index == 0:\n",
        "            break\n",
        "        hindi_char = hindi_alphabets[index+1]\n",
        "        hindi_output += hindi_char\n",
        "    print(word + ' - ' + hindi_output)\n",
        "    return hindi_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT8bibYl7CgX"
      },
      "source": [
        "def calc_accuracy(net, device = 'cpu'):\n",
        "    net = net.eval().to(device)\n",
        "    predictions = []\n",
        "    accuracy = 0\n",
        "    for i in range(len(test_data)):\n",
        "        eng, hindi = test_data[i]\n",
        "        gt = gt_rep(hindi, hindi_alpha2index, device)\n",
        "        outputs = infer(net, eng, gt.shape[0])\n",
        "        correct = 0\n",
        "        for index, out in enumerate(outputs):\n",
        "            val, indices = out.topk(1)\n",
        "            hindi_pos = indices.tolist()[0]\n",
        "            if hindi_pos[0] == gt[index][0]:\n",
        "                correct += 1\n",
        "        \n",
        "        accuracy += correct/gt.shape[0]\n",
        "    accuracy /= len(test_data)\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy1bQiORAs5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1e3c0a-4c1c-4bf8-c6f9-0b873ecf9d8e"
      },
      "source": [
        "#accuracy = calc_accuracy(net) * 100\n",
        "accuracy_attn = calc_accuracy(net_att) * 100\n",
        "#print('Accuracy w/o attention ', accuracy)\n",
        "print('Acurracy with attention', accuracy_attn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurracy with attention 73.5097609362315\n"
          ]
        }
      ]
    }
  ]
}